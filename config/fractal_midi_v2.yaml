# FractalMIDI 2.0 Configuration
# Architecture: Tempo-Aware Temporal Fractal Network (TFN)

# Model configuration
model:
  # Architecture (Hierarchical Levels from Coarse to Fine)
  architecture:
    # Sequence Length for each level (Must match downsample factors)
    # Level 0 (32):  Structure (approx 2 bars) - Coarse Planning
    # Level 1 (64):  Phrase (approx 1 bar)
    # Level 2 (128): Beat (1/4 note resolution)
    # Level 3 (512): Content (1/16 note resolution) - Full Detail
    seq_len_list: [32, 64, 128, 512]
    
    # Hidden Dimension for each level
    embed_dim_list: [512, 512, 256, 128]    
    
    # Transformer Depth (Number of Blocks)
    num_blocks_list: [8, 8, 4, 2]          
    
    # Number of Attention Heads
    num_heads_list: [8, 8, 4, 2]           
    
    attn_dropout: 0.1                      
    proj_dropout: 0.1
    init_std: 0.02
    input_channels: 2
    cond_dim: 0
    max_bar_len: 64
    compressed_dim: 64                     # Bottleneck dimension between levels
    compression_act: 'relu'

  # Generator configuration
  generator:
    # Hybrid Strategy: Structure (MAR) -> Content (AR)
    # MAR: Bidirectional context for global structure (Density/Tempo)
    # AR:  Unidirectional context with LSTM Decoder for coherent notes/chords
    generator_type_list: [ar, ar, ar, ar]
    
    scan_order: row_major # Used for AR steps if flexible order is implemented
    mask_ratio_loc: 1.0                    # MAR: mean mask ratio
    mask_ratio_scale: 0.8                   # MAR: std of mask ratio
    full_mask_prob: 0.1                     # Probability of forcing 100% mask during training
    ar_prefix_mask_ratio: 1.0               # AR: max prefix mask ratio
    num_conds: 5

  # Training settings
  training:
    grad_checkpointing: false
    v_weight: 1.0

# Training configuration
training:
  max_steps: 100000                      
  learning_rate: 1.0e-4
  weight_decay: 0.05
  warmup_steps: 2000                    
  grad_clip: 3.0
  accumulate_grad_batches: 1
  train_batch_size: 64
  val_batch_size: 8

# Optimizer
optimizer:
  lr: 1.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.05

# Scheduler
scheduler:
  schedule_type: "cosine"
  warmup_steps: 2000
  min_lr: 1.0e-6

# Data configuration
data:
  train_data: dataset/pop909/train.txt
  val_data: dataset/pop909/valid.txt
  crop_length: 512                       
  augment_factor: 10                      # Random crop augmentation factor
  pitch_shift_min: -12                    # Minimum pitch shift (semitones)
  pitch_shift_max: 12    
  cache_in_memory: true
  cache_dir: dataset/cache/pop909

# Hardware configuration
hardware:
  devices: [0, 1]
  num_workers: 24
  prefetch_factor: 4
  persistent_workers: true
  pin_memory: true
  precision: "32"

# Logging and checkpointing
logging:
  output_dir: experiments/pop909_all_ar
  log_every_n_steps: 50
  val_check_interval_steps: 1000
  checkpoint_every_n_steps: 10000
  save_top_k: 3
  log_images_every_n_steps: 1000
  num_images_to_log: 2

# Other
seed: 42
fast_dev_run: false
