# FractalMIDI 2.0 Configuration (Cleaned)
# Architecture: Tempo-Aware Temporal Fractal Network (TFN)

# Model configuration
model:
  # Architecture (Hierarchical Levels from Coarse to Fine)
  architecture:
    # Sequence Length for each level (Must match downsample factors)
    # Level 0 (32):  Structure (approx 2 bars) - Coarse Planning
    # Level 1 (64):  Phrase (approx 1 bar)
    # Level 2 (128): Beat (1/4 note resolution)
    # Level 3 (512): Content (1/16 note resolution) - Full Detail
    seq_len_list: [32, 64, 128, 512]
    
    # Hidden Dimension for each level
    embed_dim_list: [256, 256, 128, 128]    
    
    # Transformer Depth (Number of Blocks)
    num_blocks_list: [4, 4, 2, 1]          
    
    # Number of Attention Heads
    num_heads_list: [4, 4, 2, 1]           
    
    attn_dropout: 0.1                      
    proj_dropout: 0.1
    init_std: 0.02
    input_channels: 2
    cond_dim: 0
    max_bar_len: 64
    compressed_dim: 64                     # Bottleneck dimension between levels
    compression_act: 'relu'

  # Generator configuration
  generator:
    # Hybrid Strategy: Structure (MAR) -> Content (AR)
    # MAR: Bidirectional context for global structure (Density/Tempo)
    # AR:  Unidirectional context with LSTM Decoder for coherent notes/chords
    generator_type_list: [mar, mar, mar, mar]
    
    # Pitch Decoding Strategy: 'ar' (Sequential) or 'parallel' (Efficient O(1))
    pitch_generator_type: 'parallel'
    
    # Note: The following parameters are currently NOT used in the simplified FractalGen implementation
    # scan_order: row_major 
    # mask_ratio_loc: 1.0                    
    # mask_ratio_scale: 0.8                   
    # full_mask_prob: 0.1                     
    # ar_prefix_mask_ratio: 1.0               
    # num_conds: 5

  # Training settings
  training:
    grad_checkpointing: false
    v_weight: 1.0

# Training configuration
training:
  max_steps: 100000                     
  learning_rate: 3e-4
  weight_decay: 2e-3
  warmup_steps: 5000                 
  grad_clip: 3.0
  accumulate_grad_batches: 1
  train_batch_size: 32
  val_batch_size: 16

# Optimizer
optimizer:
  lr: 5e-4
  betas: [0.9, 0.95]
  weight_decay: 2e-3

# Scheduler
scheduler:
  schedule_type: "cosine"
  warmup_steps: 2000
  min_lr: 1.0e-6

# Data configuration
data:
  train_data: dataset/pop909/train.txt
  val_data: dataset/pop909/valid.txt
  crop_length: 512                       
  augment_factor: 10                      # Random crop augmentation factor
  pitch_shift_min: -12                    # Minimum pitch shift (semitones)
  pitch_shift_max: 12    
  cache_in_memory: false
  cache_dir: dataset/cache/pop909

# Hardware configuration
hardware:
  devices: [0, 1]
  num_workers: 8
  prefetch_factor: 4
  persistent_workers: true
  pin_memory: true
  precision: "32"

# Logging and checkpointing
logging:
  output_dir: experiments/pop909_mar_parallel
  log_every_n_steps: 50
  val_check_interval_steps: 5000
  checkpoint_every_n_steps: 10000
  save_top_k: 3
  log_images_every_n_steps: 5000
  num_images_to_log: 2

# Other
seed: 42
fast_dev_run: false

