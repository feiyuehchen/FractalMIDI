# Small model configuration for faster training/testing
# Reduced model size for quick experiments

# Model configuration
model:
  # Piano roll settings
  piano_roll_height: 128
  patch_size: 4
  velocity_vocab_size: 32
  
  # Architecture (4 levels: 128→16→4→1) with smaller dimensions
  img_size_list: [128, 16, 4, 1]
  embed_dim_list: [256, 256, 128, 64]     # Half the default size
  num_blocks_list: [8, 4, 2, 1]          # Fewer transformer blocks
  num_heads_list: [4, 2, 1, 1]           # Fewer attention heads
  attn_dropout: 0.1                      # More dropout for smaller model
  proj_dropout: 0.1
  init_std: 0.02
  
  # Generator configuration
  generator_types: [ar, ar, ar, ar]
  scan_order: column_major
  mask_ratio_loc: 1.0
  mask_ratio_scale: 0.5
  num_conds: 5
  
  # Training settings
  grad_checkpointing: false
  v_weight: 1.0

# Training configuration
training:
  max_steps: 30000                       # AR needs more steps
  learning_rate: 1.0e-4
  weight_decay: 0.05
  warmup_steps: 2000                     # Longer warmup for stability
  grad_clip: 3.0
  accumulate_grad_batches: 2
  train_batch_size: 8
  val_batch_size: 8

# Data configuration
data:
  train_data: dataset/train.txt
  val_data: dataset/valid.txt
  crop_length: 256                       
  augment_factor: 5                       # Random crop augmentation factor
  pitch_shift_min: -12                    # Minimum pitch shift (semitones)
  pitch_shift_max: 12    
  cache_in_memory: true
  cache_dir: null

# Hardware configuration
hardware:
  devices: [0, 1]
  num_workers: 24
  prefetch_factor: 4
  persistent_workers: true
  pin_memory: true
  precision: "32"

# Logging and checkpointing
logging:
  output_dir: outputs/fractalgen_small
  log_every_n_steps: 50
  val_check_interval_steps: 2000
  checkpoint_every_n_steps: 5000
  save_top_k: 3
  log_images_every_n_steps: 2000
  num_images_to_log: 2

# Other
seed: 42
fast_dev_run: false

